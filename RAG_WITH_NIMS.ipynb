{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab245522",
   "metadata": {},
   "source": [
    "# RAG with NVIDIA NIM Microservices\n",
    "\n",
    "Welcome to this lab! In this notebook, you'll learn how to build a Retrieval-Augmented Generation (RAG) pipeline using NVIDIA NIM microservices, hosted on the [NVIDIA API Catalog](https://build.nvidia.com/models).\n",
    "\n",
    "We'll walk through the following:\n",
    "\n",
    "- Connecting to an NVIDIA-hosted LLM (Llama3.1-8b-instruct) using [LangChain's NVIDIA AI Endpoints integration](https://python.langchain.com/docs/integrations/chat/nvidia_ai_endpoints/).\n",
    "- Creating a vector store from custom documents using FAISS and GPU-accelerated NVIDIA-hosted embedding models (NV-Embed-QA).\n",
    "- Running intelligent, context-aware chat chains over the embedded documents.\n",
    "\n",
    "This lab uses real-world data from NVIDIA Documentation for NIMS to demonstrate a practical RAG implementation.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "---\n",
    "\n",
    "##  Architecture Diagram\n",
    "\n",
    "<img src=\"./RAG_WITH_NIMS.png\" alt=\"RAG Architecture\" width=\"600\"/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:**  \n",
    "To run this notebook, you’ll need an account on both the [NVIDIA API Catalog (build.nvidia.com)](https://build.nvidia.com/) and the [NVIDIA NGC Catalog](https://catalog.ngc.nvidia.com/).\n",
    "\n",
    "- **NVIDIA API Catalog** is a platform where you can access hosted NVIDIA models as API endpoints, including LLMs, embedding models, and more. You’ll generate a personal API key here to authenticate requests.\n",
    "- **NGC (NVIDIA GPU Cloud) Catalog** provides access to enterprise-grade AI software, models, and containers. It’s required for enabling API access to certain models via NIM.\n",
    "\n",
    "Make sure you're logged in to both and have generated your API key from the [NVIDIA API Keys page](https://build.nvidia.com/settings/api-keys) before continuing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf3b96",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "We’ll begin by installing the necessary Python packages for building our RAG pipeline.\n",
    "\n",
    "- `langchain-community` and `langchain-nvidia-ai-endpoints` are used to integrate with NVIDIA’s hosted LLMs via LangChain.\n",
    "- `faiss-cpu` is used for creating a local vector store. If you’re running on a GPU, you can optionally use `faiss-gpu`.\n",
    "- `beautifulsoup4` is used to parse HTML content from web pages, which we’ll embed later.\n",
    "\n",
    "> Note: Make sure your `pip` is up to date before installing the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5966ea5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install langchain-community==0.2.5 --q\n",
    "!pip install langchain-nvidia-ai-endpoints==0.1.2 --q\n",
    "!pip install faiss-cpu --q # or faiss-gpu if you have a GPU\n",
    "!pip install beautifulsoup4  --q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14bab53",
   "metadata": {},
   "source": [
    "> Note: Make sure you restart the kernal after installing the pip packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ba025",
   "metadata": {},
   "source": [
    "## Set Up Your NVIDIA API Key\n",
    "To authenticate with the NVIDIA API Catalog, you need to set your personal API key as an environment variable. This key allows you to access the hosted models via LangChain.\n",
    "\n",
    "If you haven’t already, generate your key from the [NVIDIA API Keys page](https://build.nvidia.com/settings/api-keys), and replace the placeholder below with your actual key.\n",
    "\n",
    "> **Important:** Never share your API key publicly or commit it to source control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0a084d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['NVIDIA_API_KEY']='<Your NGC_CLI_KEY_HERE>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a45862d",
   "metadata": {},
   "source": [
    "### Verify the API Key\n",
    "Let’s confirm that the `NVIDIA_API_KEY` environment variable is set correctly. The command below should print your API key (or part of it, depending on your environment setup). If it returns an empty line, double-check that you set it properly in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fb00f4-2ad1-4c99-812e-cb03d3a01ef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo $NVIDIA_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c6fb2",
   "metadata": {},
   "source": [
    "## Test API Call to Llama Model\n",
    "Let’s test the connection to the NVIDIA-hosted Llama model by making a simple API call. This will send a prompt (\"What is API?\") to the model and retrieve a response.\n",
    "\n",
    "If successful, the model will return an answer based on the question, and you'll be able to confirm that your API key and setup are working correctly.\n",
    "\n",
    "> If you encounter any errors, make sure that your API key is correctly set and has the proper permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bf04b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl --request POST \\\n",
    "  --url https://integrate.api.nvidia.com/v1/chat/completions \\\n",
    "  --header \"Authorization: Bearer $NVIDIA_API_KEY\" \\\n",
    "  --header \"Accept: application/json\" \\\n",
    "  --header \"Content-Type: application/json\" \\\n",
    "  --data '{\"messages\": [{\"role\": \"user\", \"content\": \"what is api?\"}], \\\n",
    "\"model\": \"meta/llama-3.1-8b-instruct\", \"max_tokens\": 24, \"top_p\": 0.7, \"temperature\": 0.2}'\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584e3b1",
   "metadata": {},
   "source": [
    "## Initialize LLM with LangChain\n",
    "Now, we'll use the `ChatNVIDIA` class from the `langchain_nvidia_ai_endpoints` package to initialize the connection to the NVIDIA Llama model. We specify the base URL, model type, and several parameters like `temperature`, `max_tokens`, and `top_p` to control the model's output.\n",
    "\n",
    "In this test, we’ll ask the model, \"What is the capital of France?\" and print the result.\n",
    "\n",
    "If everything is set up correctly, the model should return \"Paris\" as the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35baa8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(base_url=\"https://integrate.api.nvidia.com/v1\", model=\"meta/llama-3.1-8b-instruct\", temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "result = llm.invoke(\"What is the capital of France?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95b2753",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "\n",
    "We’ll import a variety of libraries needed for this lab:\n",
    "\n",
    "- **LangChain Components**: These include chains like `ConversationalRetrievalChain`, `LLMChain`, and `QA_PROMPT`, which will be used to handle question-answering over our embedded documents.\n",
    "- **FAISS**: This is used to create and manage our vector store, where we will store and query document embeddings.\n",
    "- **Text Splitter**: We’ll use `RecursiveCharacterTextSplitter` to break down text into smaller chunks for embedding.\n",
    "- **ChatNVIDIA and NVIDIAEmbeddings**: These allow us to interact with the NVIDIA LLM and to generate embeddings using NVIDIA's API.\n",
    "\n",
    "These libraries will help us build the RAG pipeline with both retrieval and generation capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42bf2619-1ca3-4477-82b8-88c240dd87ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f8822",
   "metadata": {},
   "source": [
    "## HTML Document Loader\n",
    "\n",
    "In this step, we define a function `html_document_loader` that retrieves the plain text from an HTML document at a given URL.\n",
    "\n",
    "The function does the following:\n",
    "- Makes an HTTP request to load the HTML content using `requests`.\n",
    "- Uses **BeautifulSoup** to parse the HTML, removing `script` and `style` tags that aren't relevant for text extraction.\n",
    "- Extracts and cleans up the text by removing excess whitespace.\n",
    "\n",
    "This function will be useful for fetching and processing web page content, which we’ll later embed into the vector store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8097819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def html_document_loader(url: Union[str, bytes]) -> str:\n",
    "    \"\"\"\n",
    "    Loads the HTML content of a document from a given URL and return it's content.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the document.\n",
    "\n",
    "    Returns:\n",
    "        The content of the document.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error while making the HTTP request.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url} due to exception {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Create a Beautiful Soup object to parse html\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Remove script and style tags\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        # Get the plain text from the HTML document\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # Remove excess whitespace and newlines\n",
    "        text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} while loading document\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2d1e0",
   "metadata": {},
   "source": [
    "## Create Embeddings from Web Pages\n",
    "\n",
    "The `create_embeddings` function performs the following steps:\n",
    "\n",
    "1. **Fetch Web Content**: We define a list of URLs (in this case, NVIDIA’s documentation) and use the previously defined `html_document_loader` function to fetch and process the HTML content.\n",
    "2. **Text Splitting**: We use **RecursiveCharacterTextSplitter** to break the raw text into chunks, ensuring that each chunk is within a specified size for easier processing and embedding.\n",
    "3. **Generate Embeddings**: After splitting the text into manageable chunks, we call `index_docs` to generate embeddings using the provided `embedding_model` and store them in a specified directory.\n",
    "\n",
    "This process converts the textual data from web pages into vector embeddings that can later be queried in a vector store for RAG-based question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa8900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_embeddings(embedding_path: str = \"./data/nv_embedding\", embedding_model):\n",
    "\n",
    "    embedding_path = \"./data/nv_embedding\"\n",
    "    print(f\"Storing embeddings to {embedding_path}\")\n",
    "\n",
    "    # List of web pages containing NVIDIA Triton technical documentation\n",
    "    urls = [\n",
    "         \"https://docs.nvidia.com/nim/large-language-models/latest/introduction.html\",\n",
    "    ]\n",
    "\n",
    "    documents = []\n",
    "    for url in urls:\n",
    "        document = html_document_loader(url)\n",
    "        documents.append(document)\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "    )\n",
    "    texts = text_splitter.create_documents(documents)\n",
    "    index_docs(url, embedding_model=embedding_model, splitter=text_splitter, documents=texts, dest_embed_dir=embedding_path)\n",
    "    print(\"Generated embedding successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e2097",
   "metadata": {},
   "source": [
    "## Index Documents and Create Embeddings\n",
    "\n",
    "The `index_docs` function handles the creation and storage of embeddings for the given documents.\n",
    "\n",
    "Here’s what happens in the function:\n",
    "1. **Text Splitting**: The function splits the document into smaller chunks using the provided `splitter`.\n",
    "2. **Embedding Generation**: For each chunk, embeddings are generated using the specified `embedding_model` (e.g., NV-Embed-QA).\n",
    "3. **FAISS Vector Store**: \n",
    "   - If the destination directory for embeddings already exists, it loads the existing FAISS vector store and updates it with the new embeddings.\n",
    "   - If the directory doesn’t exist, it creates a new FAISS index, stores the embeddings, and saves it locally.\n",
    "\n",
    "The embeddings are stored in the specified directory, ready for querying later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e6a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def index_docs(url: Union[str, bytes], embedding_model, splitter, documents: List[str], dest_embed_dir) -> None:\n",
    "    \"\"\"\n",
    "    Split the document into chunks and create embeddings for the document\n",
    "\n",
    "    Args:\n",
    "        url: Source url for the document.\n",
    "        splitter: Splitter used to split the document\n",
    "        documents: list of documents whose embeddings needs to be created\n",
    "        dest_embed_dir: destination directory for embeddings\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    embeddings = embedding_model\n",
    "\n",
    "    for document in documents:\n",
    "        texts = splitter.split_text(document.page_content)\n",
    "\n",
    "        # metadata to attach to document\n",
    "        metadatas = [document.metadata]\n",
    "\n",
    "        # create embeddings and add to vector store\n",
    "        if os.path.exists(dest_embed_dir):\n",
    "            update = FAISS.load_local(folder_path=dest_embed_dir, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "            update.add_texts(texts, metadatas=metadatas)\n",
    "            update.save_local(folder_path=dest_embed_dir)\n",
    "        else:\n",
    "            docsearch = FAISS.from_texts(texts, embedding=embeddings, metadatas=metadatas)\n",
    "            docsearch.save_local(folder_path=dest_embed_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44650d71",
   "metadata": {},
   "source": [
    "## Generate and Store Embeddings\n",
    "\n",
    "Here, we initialize the **NVIDIAEmbeddings** model with the \"NV-Embed-QA\" model and the truncation method set to \"END\". This embedding model is specifically designed for efficient question answering tasks.\n",
    "\n",
    "After initializing the model, we call the `create_embeddings` function to fetch the content from the provided URLs, split the text into chunks, and generate embeddings for each chunk. These embeddings are then stored in a local directory for later use.\n",
    "\n",
    "The embeddings are key to enabling effective retrieval from the vector store in our RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db1c5c-f515-460f-bf23-5d68f195e52b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"END\")\n",
    "create_embeddings(embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9f5e2",
   "metadata": {},
   "source": [
    "## Load Pre-generated Embeddings\n",
    "\n",
    "In this step, we load the pre-generated embeddings from the local FAISS vector store. The `FAISS.load_local` method is used to load the embeddings from the specified folder (`embedding_path`), which we created earlier.\n",
    "\n",
    "We pass the `embedding_model` to ensure the embeddings are compatible with our model, and the `allow_dangerous_deserialization=True` argument is included to handle potential deserialization issues.\n",
    "\n",
    "Now that the embeddings are loaded, we can perform efficient similarity searches against them in the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d55bb79e-5bb6-409d-8ead-3c3006aeb2ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embed documents\n",
    "embedding_path = \"./data/nv_embedding\"\n",
    "docsearch = FAISS.load_local(folder_path=embedding_path, embeddings=embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7614e948-aab6-40d5-bf14-4f8ba99b1329",
   "metadata": {},
   "source": [
    "## Set Up Conversational Retrieval Chain\n",
    "\n",
    "In this step, we initialize the **ConversationalRetrievalChain** to enable question answering over our document store. Here’s what happens:\n",
    "\n",
    "1. **LLM Initialization**: We use the **ChatNVIDIA** class to set up the Llama 3.1 model as our language model, specifying parameters like `temperature`, `max_tokens`, and `top_p` for controlling the output generation.\n",
    "2. **Memory**: A **ConversationBufferMemory** is created to maintain the chat history across multiple interactions, allowing for more contextualized responses.\n",
    "3. **QA Prompt**: We define the prompt template (**`QA_PROMPT`**) to structure the model's responses in a question-answer format.\n",
    "4. **QA Chain**: We load the question-answering chain with the LLM and the QA prompt.\n",
    "5. **Conversational Chain**: The **ConversationalRetrievalChain** combines the LLM and the document retriever (FAISS in our case). This chain will take user input, retrieve relevant documents, and generate answers based on both the documents and the chat history.\n",
    "\n",
    "This setup allows us to perform intelligent, context-aware Q&A on our embedded documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94b49b4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(base_url=\"https://integrate.api.nvidia.com/v1\", model=\"meta/llama3-8b-instruct\", temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa_prompt=QA_PROMPT\n",
    "\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QA_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f2240",
   "metadata": {},
   "source": [
    "## Test the Retrieval Chain\n",
    "\n",
    "Now that we've set up the conversational retrieval chain, we test it by asking the model a question: **\"What are NIMS?\"**.\n",
    "\n",
    "- The model will retrieve relevant documents from the FAISS vector store and generate an answer based on both the retrieved documents and the chat history.\n",
    "- The result is printed, which should provide a response to the question.\n",
    "\n",
    "This step demonstrates the full functionality of the Retrieval-Augmented Generation (RAG) pipeline: combining document retrieval with generative language models to answer user queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add6e2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are NIMS\"\n",
    "result = qa.invoke({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1cddd",
   "metadata": {},
   "source": [
    "## Test the Retrieval Chain with Another Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62654a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"Brief about its architecture\"\n",
    "result = qa.invoke({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178ac86",
   "metadata": {},
   "source": [
    "This query demonstrates how the model can adapt to different types of questions, including those related to specific topics such as applications, based on the documents it has been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e058e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What about its applications?\"\n",
    "result = qa.invoke({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907dbaa",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this lab, we demonstrated how to build a simple Retrieval-Augmented Generation (RAG) pipeline using **NVIDIA NIM microservices**. We integrated NVIDIA-hosted LLMs and embedding models via LangChain, created a local FAISS vector store using content from the HPE product documentation site, and built a conversational retrieval chain to interact with that knowledge base.\n",
    "\n",
    "This setup showcases the power of combining hosted foundation models with custom data to deliver intelligent, context-aware responses.\n",
    "\n",
    "### What's Next?\n",
    "- Trying different NIM-hosted models.\n",
    "- Explore the [NVIDIA API Catalog](https://build.nvidia.com/models)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
